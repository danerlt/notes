# OpenLLM

## ä»€ä¹ˆæ˜¯ OpenLLM

> OpenLLM æ˜¯ä¸€ä¸ªå¼€æºå¹³å°ï¼Œæ—¨åœ¨ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²å’Œæ“ä½œã€‚å€ŸåŠ© OpenLLMï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•å¼€æº LLM ä¸Šè¿è¡Œæ¨ç†ï¼Œå°†å…¶éƒ¨ç½²åœ¨äº‘ç«¯æˆ–æœ¬åœ°ï¼Œå¹¶æ„å»ºå¼ºå¤§çš„ AI åº”ç”¨ç¨‹åºã€‚

OpenLLMæ˜¯ä¸ºAIåº”ç”¨å¼€å‘äººå‘˜è®¾è®¡çš„ï¼Œç”¨äºæ„å»ºåŸºäºLLMçš„ç”Ÿäº§å°±ç»ªåº”ç”¨ç¨‹åºã€‚å®ƒæä¾›äº†ä¸€å¥—å…¨é¢çš„å·¥å…·å’ŒåŠŸèƒ½ï¼Œç”¨äºå¾®è°ƒã€æœåŠ¡ã€éƒ¨ç½²å’Œç›‘æ§è¿™äº›æ¨¡å‹ï¼Œç®€åŒ–äº†LLMçš„ç«¯åˆ°ç«¯éƒ¨ç½²å·¥ä½œæµç¨‹ã€‚


ä¸»è¦åŠŸèƒ½ï¼š

- ğŸš‚ æœ€å…ˆè¿›çš„ LLMï¼šå¯¹å„ç§å¼€æº LLM å’Œæ¨¡å‹è¿è¡Œæ—¶çš„é›†æˆæ”¯æŒï¼ŒåŒ…æ‹¬ä½†ä¸é™äº Llama 2ã€StableLMã€Falconã€Dollyã€Flan-T5ã€ChatGLM å’Œ StarCoderã€‚
- ğŸ”¥ çµæ´»çš„ APIï¼šä½¿ç”¨ä¸€ä¸ªå‘½ä»¤å°±å¯ä»¥ä¸º LLM æä¾› RESTful API æˆ– gRPC æ¥å£ã€‚å¯ä»¥ä½¿ç”¨ Web UIã€CLIã€Python/JavaScript å®¢æˆ·ç«¯æˆ–ä»»ä½• HTTP å®¢æˆ·ç«¯ä¸å¤§æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
- â›“ï¸ è‡ªç”±æ„å»ºï¼šå¯¹ LangChainã€BentoMLã€LlamaIndexã€OpenAI ç«¯ç‚¹å’Œ Hugging Face æä¾›å¾ˆå¥½æ”¯æŒï¼Œå¯ä»¥é€šè¿‡å°† LLM ä¸å…¶ä»–æ¨¡å‹å’ŒæœåŠ¡ç»„åˆæ¥è½»æ¾åˆ›å»ºè‡ªå·±çš„ AI åº”ç”¨ç¨‹åºã€‚
- ğŸ¯ ç®€åŒ–éƒ¨ç½²ï¼šè‡ªåŠ¨ç”Ÿæˆ LLM æœåŠ¡å¯¹åº”çš„ Docker æ˜ åƒæˆ–é€šè¿‡ [BentoCloud](https://l.bentoml.com/bento-cloud) éƒ¨ç½²ä¸º serverless æœåŠ¡ï¼Œå®ƒå¯ä»¥è½»æ¾ç®¡ç† GPU èµ„æºã€æ ¹æ®æµé‡è¿›è¡Œæ‰©å±•å¹¶ç¡®ä¿æˆæœ¬æ•ˆç›Šã€‚
- ğŸ¤–ï¸ åŠ è½½è‡ªå·±çš„ LLMï¼šæ ¹æ®å…·ä½“éœ€æ±‚å¾®è°ƒä»»ä½• LLM ã€‚å¯ä»¥åŠ è½½ LoRA å±‚æ¥å¾®è°ƒæ¨¡å‹ï¼Œä»¥è·å¾—ç‰¹å®šä»»åŠ¡çš„æ›´é«˜å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚ç»Ÿä¸€çš„æ¨¡å‹å¾®è°ƒ API ( LLM.tuning() ) å³å°†æ¨å‡ºã€‚
- âš¡ é‡åŒ–ï¼šä½¿ç”¨ LLM.int8ã€SpQR (int4)ã€AWQã€GPTQ å’Œ SqueezeLLM ç­‰é‡åŒ–æŠ€æœ¯ä»¥æ›´å°‘çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬è¿è¡Œæ¨ç†ã€‚
- ğŸ“¡ æµå¼ä¼ è¾“ï¼šæ”¯æŒé€šè¿‡ Server-Sentï¼ˆSSEï¼‰å¯¹ token è¿›è¡Œ æµå¼ä¼ è¾“ã€‚å¯ä»¥ä½¿ç”¨ `/v1/generate_stream` æµå¼ä¼ è¾“æ¥è‡ª LLM çš„å“åº”ã€‚
- ğŸ”„ è¿ç»­æ‰¹å¤„ç†ï¼šé€šè¿‡ vLLM æ”¯æŒè¿ç»­æ‰¹å¤„ç†ï¼Œä»¥æé«˜æ€»ååé‡ã€‚

## OpenLLM æœ‰ä»€ä¹ˆç”¨

## OpenLLM å¦‚ä½•ä½¿ç”¨

å®‰è£…å‘½ä»¤
```bash
pip install openllm
```

éªŒè¯å®‰è£…ï¼š
```bash
$ openllm -h

Usage: openllm [OPTIONS] COMMAND [ARGS]...

   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
  â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
   â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•.

  An open platform for operating large language models in production.
  Fine-tune, serve, deploy, and monitor any LLMs with ease.

Options:
  -v, --version  Show the version and exit.
  -h, --help     Show this message and exit.

Commands:
  build       Package a given models into a BentoLLM.
  import      Setup LLM interactively.
  models      List all supported models.
  prune       Remove all saved models, (and optionally bentos) built with OpenLLM locally.
  query       Query a LLM interactively, from a terminal.
  start       Start a LLMServer for any supported LLM.
  start-grpc  Start a gRPC LLMServer for any supported LLM.

Extensions:
  build-base-container  Base image builder for BentoLLM.
  dive-bentos           Dive into a BentoLLM.
  get-containerfile     Return Containerfile of any given Bento.
  get-prompt            Get the default prompt used by OpenLLM.
  list-bentos           List available bentos built by OpenLLM.
  list-models           This is equivalent to openllm models...
  playground            OpenLLM Playground.
```


ä»¥ chatglm æ¨¡å‹ä¸ºä¾‹ï¼Œå¯åŠ¨æ¨¡å‹ï¼š

å®‰è£…ä¾èµ–ï¼š
```bash
pip install "openllm[chatglm]"
pip install "openllm[vllm]"
```

å¯åŠ¨å‘½ä»¤ï¼š
```bash
TRUST_REMOTE_CODE=True openllm start /yourdir/models/chatglm3-6b  --backend vllm -p 3333
```

è¿è¡Œè¿‡ç¨‹ä¸­æŠ¥é”™ï¼š
![](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/2024-01-05-kHvgUD.png)

éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ `OPENBLAS_NUM_THREADS=1`

```bash
OPENBLAS_NUM_THREADS=1 TRUST_REMOTE_CODE=True nohup openllm start /yourdir/models/chatglm3-6b  --backend vllm -p 3333 >> chatglm.log 2>&1
```

å¯åŠ¨æˆåŠŸåï¼Œè®¿é—® `http://localhost:3333` å³å¯çœ‹åˆ°æ¨¡å‹çš„æ¨ç†ç»“æœã€‚

ä½¿ç”¨ chatglm æ¨¡å‹å¯åŠ¨æˆåŠŸä¹‹åæ— æ³•ä½¿ç”¨ï¼Œåˆ‡æ¢æˆ baichuan æ¨¡å‹ï¼š

```bash
cmd="env OPENBLAS_NUM_THREADS=1 TRUST_REMOTE_CODE=True openllm start /yourdir/models/Baichuan2-13B-Chat  --backend vllm -p 3334"
nohup $cmd > baichuan.log 2>&1;
```

è¿è¡ŒæŠ¥é”™ `RuntimeError: The NVIDIA driver on your system is too old (found version 11040)`ï¼š

![](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/2024-01-05-riCyfc.png)

æ¢ç”¨ bakend ä¸º pytorch å†è¯•ä¸€ä¸‹ï¼š

```bash
cmd="env OPENBLAS_NUM_THREADS=1 TRUST_REMOTE_CODE=True openllm start /yourdir/models/Baichuan2-13B-Chat  --backend pt -p 3334"
nohup $cmd > baichuan.log 2>&1;
```

è¿è¡ŒæŠ¥é”™ `torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 39.59 GiB total capacity; 20.60 GiB already allocated; 26.19 MiB free; 20.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
`


## å‚è€ƒé“¾æ¥

- [OpenLLM Github](https://github.com/bentoml/OpenLLM)
- [ä½¿ç”¨ OpenLLM æ„å»ºå’Œéƒ¨ç½²å¤§æ¨¡å‹åº”ç”¨](https://mp.weixin.qq.com/s/QPYZXyv8FzdcXH1vX7iYSA)
