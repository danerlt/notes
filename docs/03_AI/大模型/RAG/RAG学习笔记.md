# RAG（Retrieval-Augmented Generation）学习笔记

## 论文：Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

论文名称： Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks（知识密集型 NLP 任务的检索增强生成）

论文地址：[ https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)


### 提出的问题是什么

实验表明预训练语言模型能够从海量数据中学习到广泛的世界知识，**这些知识以参数的形式存储在模型中，经过适当的微调就能在下游任务中取得SOTA表现，而不需要访问或检索额外的外部知识库**。但是，这类通用语言模型在处理**知识密集型(knowledge-intensive)** 任务时仍旧存在一定的局限性，常常落后于面向特定任务的模型结构。预训练模型容量再大也无法记住所有的知识，一旦训练好了很难方便的扩展和修改知识。

问题：知识密集型任务中，如何提高预训练模型在知识密集型任务上的表现，并且扩展性更好？

### 用的什么解决方法

给大模型添加外部知识库。在当前的知识密集型任务的研究中，具有实用价值的模型基本都依赖于外部知识库，**开放域问答系统(Open-domain QA)** 是最经典也最重要的知识密集型任务之一，目前的SOTA模型基本都包含两个模块：文档检索器和文档生成器，前者负责检索召回和查询相关的文档，后者负责从这些文档中编辑或抽取出答案片段。**这种检索-抽取范式类似于开卷考试，而单纯依靠模型参数来保存知识类似于闭卷考试**，前者当然要简单可靠得多。



### 成果如何

在开放域问答任务中，作者对比了RAG和一些流行的抽取式QA模型，包括依赖于非参数知识(Open Book)的REALM和DPR，以及仅依靠参数知识(Closed Book)的生成式模型T5，评估指标为EM分数。在其余三项任务中，作者还对比测试了BART模型和利用了文档监督信息的SOTA模型。

![img](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/v2-2266b8891c7bc9635e50050f3d0d07e6_720w.webp)



下面是一些主要结论：

-   总体来说，Open Book范式比Closed Book范式的表现好不少，因此引入外部知识库对开放域问答任务来说还是很重要的，而**RAG很好地将seq2seq模型的灵活性和检索模型的高效性结合了起来**，并且不像REALM，RAG的训练成本更小，另外RAG的检索器虽然是用DPR的检索器初始化的，但原始的DPR模型后续采用了BERT来对文档做重排和答案抽取，而RAG表明这两者是不必要的。

-   虽然可以直接从文档中抽取答案片段，但直接生成答案有一些额外的好处，比如有些文档并不直接包含整个答案，但包含答案的线索，**这些线索就能帮助模型生成更正确的答案**，而这对抽取式模型来说是做不到的。在一些极端情况下，比如被检索到的文档全都不包含正确答案，RAG也能生成相对合理的答案，此时RAG借助的就是生成器中存储的参数知识，而REALM这类抽取式QA模型就无法回答这些问题。在NQ数据集中，RAG对于这类问题的正确率是11.8%，而抽取式模型的正确率是0%。

-   作者对BART和RAG模型在问题生成任务上的表现进行了人工评估，如左图所示，评估结果表明RAG生成的问题更符合事实(factual)，也更具体(specific)，而右图计算了不同的tri-grams与所有tri-grams的比值，该比值能够反映生成的多样性，计算结果和样本观察均表明RAG的生成结果是更多样化的。

    ![img](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/v2-90f3d16f2b1a9f6aa0b24459c88d3e15_720w.webp)

    ![img](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/v2-c2b91a3920a0a2a1cd81ad4bc213526d_720w.webp)

-   另外，我们可以发现在问题生成任务中RAG-Token的表现比RAG-Sequence更好，这实际上得益于前者在生成时可以关注到多个文档，从而生成信息更丰富的问题，下面的一个例子就表明了RAG-Token在生成不同的单词时，不同文档的后验概率是不同的。有趣的是，在生成某个实体的第一个词之后，该实体对应的文档的后验概率就回归正常了，这表明**生成器依靠参数知识完全有能力补全后续部分，文档信息仅仅起到了提示和引导的作用**，因此整个RAG模型主要依靠的还是参数知识，而在生成实体时非参数知识才会起到作用。

    ![img](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/v2-523842e23b2e0fba43fdc3b990dda037_720w.webp)

### 原理是啥

-   RAG 的思路其实很简单, 我们知道, 生成模型通过建模条件分布:

    $$
    p(y|x)
    $$

    并从中不断地采样序列.

-   不依赖检索的生成模型可以理解为 $$ x→y $$ 的过程, 现在我们希望赋予模型检索的能力, 即希望通过如下方式进行生成

    $$
    x→(x,z)→y
    $$

    即模型需要先通过 $$x$$ 检索得到 $$z$$ 并一起生成最后的 $$y$$.

-   实际上, 就是 (此处的积分是勒贝格积分)

    $$
    \boldsymbol{y} \sim  p(\boldsymbol{y}|\boldsymbol{x}) = \int p_{\theta}(\boldsymbol{y}|\boldsymbol{z}, \boldsymbol{x}) p_{\eta}(\boldsymbol{z}|\boldsymbol{x}) \mathrm{d} \boldsymbol{z},
    $$
    
-   其中 $p_{\theta}, p_{\eta}$ 是我们构建的两个条件模型:

    ![img](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/1603215-20230718143654762-686920493.png)

解释一下上图： 这个图有两个部分，一个是 Retriever，一个是Generator。Retriever有Query Encodeer和Document Index组成。

联合一个预训练的 retriever (Query Encoder + Document Index) 和一个预训练的 seq2seq 模型 (Generator) 并且进行了端到端的微调。对应查询 $x$，使用MIPS 算法找到 top-K 个文档  $z_i$ ，为了预测最终结果$y$，将 $z$ 当作自变量，并且对与不同的文档

其中有两种实际的模型：

-   **RAG-Sequence:** 即通过检索后的文档生成完整的序列, 假设我们检索出 (相同的) Tok-k 最相关的文档 (注意这些文档), 我们可以用如下方式近似 

    $$
    p(\boldsymbol{y} \mid \boldsymbol{x}) \approx \sum_{\boldsymbol{z} \in \operatorname{top}-k(p(\cdot \mid \boldsymbol{x}))} p_{\eta}(\boldsymbol{z} \mid \boldsymbol{x}) p_{\theta}(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{z})=\sum_{\boldsymbol{z} \in \operatorname{top}-k(p(\cdot \mid \boldsymbol{x}))} p_{\eta}(\boldsymbol{z} \mid \boldsymbol{x}) \prod_{i=1}^{N} p_{\theta}\left(y_{i} \mid \boldsymbol{x}, \boldsymbol{z}, y_{1: i-1}\right)
    $$

-   **RAG-Token:** 采用的是一种迭代的方式, 对于第 $$i$$ 个需要预测的 Token, 它

    $$
    p(y_i|\boldsymbol{x}; \boldsymbol{y}_{1:i-1}) 
    \approx \sum_{\boldsymbol{z} \in \text{top-}k (p(\cdot|\boldsymbol{x}, \boldsymbol{y}_{1:i-1}))} p_{\eta}(\boldsymbol{z}|\boldsymbol{x},  \boldsymbol{y}_{1:i-1}) p_{\theta}(y_i|\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{y}_{1:i-1})
    $$

    于是

    $$
    p(\boldsymbol{y}|\boldsymbol{x}) 
    \approx \prod_{i=1}^N \sum_{\boldsymbol{z} \in \text{top-}k (p(\cdot|\boldsymbol{x}, \boldsymbol{y}_{1:i-1}))} p_{\eta}(\boldsymbol{z}|\boldsymbol{x},  \boldsymbol{y}_{1:i-1}) p_{\theta}(y_i|\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{y}_{1:i-1})
    $$
    

这相当于, 每一个 Token 的生成我们进行一次文档的检索 (而 **RAG-Sequence:** 则是只检索一次).
    

从公式对这两种模型进行理解，RAG-Sequence模型先计算每个文档条件下回答词元的概率分布，再连乘得到每个文档条件下回答的概率分布，最后再求和得到所有最相关文档条件下回答的概率分布，而RAG-Token模型先计算每个文档条件下回答词元的概率分布，再求和得到所有最相关文档条件下回答词元的概率分布，最后再连乘得到所有最相关文档条件下回答的概率分布。



## 样例

基于上面的论文，有两个已经比较流行的样例。

### Langchain-Chatchat

>    一种利用 [langchain](https://github.com/hwchase17/langchain) 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。

项目地址： [https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)

原理如下：

![Langchain工作原理](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/Langchain%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png)

从文档处理角度：

![实现原理图2](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/langchain.png)

### dify

>   Dify 是一个 LLM 应用开发平台，已经有超过 10 万个应用基于 Dify.AI 构建。它融合了 Backend as Service 和 LLMOps 的理念，涵盖了构建生成式 AI 原生应用所需的核心技术栈，包括一个内置 RAG 引擎。使用 Dify，你可以基于任何模型自部署类似 Assistants API 和 GPTs 的能力。

项目地址： [https://github.com/langgenius/dify](https://github.com/langgenius/dify)



### 混合检索

dify中使用了混合检索来提高检索的准确度。

RAG 检索环节中的主流方法是向量检索，即语义相关度匹配的方式。技术原理是通过将外部知识库的文档先拆分为语义完整的段落或句子，并将其转换（Embedding）为计算机能够理解的一串数字表达（多维向量），同时对用户问题进行同样的转换操作。

计算机能够发现用户问题与句子之间细微的语义相关性，比如 “猫追逐老鼠” 和 “小猫捕猎老鼠” 的语义相关度会高于 “猫追逐老鼠” 和 “我喜欢吃火腿” 之间的相关度。在将相关度最高的文本内容查找到后，RAG 系统会将其作为用户问题的上下文一起提供给大模型，帮助大模型回答问题。

除了能够实现复杂语义的文本查找，向量检索还有其他的优势：

-   相近语义理解（如老鼠/捕鼠器/奶酪，谷歌/必应/搜索引擎）
-   多语言理解（跨语言理解，如输入中文匹配英文）
-   多模态理解（支持文本、图像、音视频等的相似匹配）
-   容错性（处理拼写错误、模糊的描述）

虽然向量检索在以上情景中具有明显优势，但有某些情况效果不佳。比如：

-   搜索一个人或物体的名字（例如，伊隆·马斯克，iPhone 15）
-   搜索缩写词或短语（例如，RAG，RLHF）
-   搜索 ID（例如， `gpt-3.5-turbo` ， `titan-xlarge-v1.01` ）

而上面这些的缺点恰恰都是传统关键词搜索的优势所在，传统关键词搜索擅长：

-   精确匹配（如产品名称、姓名、产品编号）
-   少量字符的匹配（通过少量字符进行向量检索时效果非常不好，但很多用户恰恰习惯只输入几个关键词）
-   倾向低频词汇的匹配（低频词汇往往承载了语言中的重要意义，比如“你想跟我去喝咖啡吗？”这句话中的分词，“喝”“咖啡”会比“你”“想”“吗”在句子中承载更重要的含义）

对于大多数文本搜索的情景，首要的是确保潜在最相关结果能够出现在候选结果中。向量检索和关键词检索在检索领域各有其优势。混合搜索正是结合了这两种搜索技术的优点，同时弥补了两方的缺点。

在混合检索中，你需要在数据库中提前建立向量索引和关键词索引，在用户问题输入时，分别通过两种检索器在文档中检索出最相关的文本。

![hybrid-search](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/hybrid-search.png)



“混合检索”实际上并没有明确的定义，本文以向量检索和关键词检索的组合为示例。如果我们使用其他搜索算法的组合，也可以被称为“混合检索”。比如，我们可以将用于检索实体关系的知识图谱技术与向量检索技术结合。

不同的检索系统各自擅长寻找文本（段落、语句、词汇）之间不同的细微联系，这包括了精确关系、语义关系、主题关系、结构关系、实体关系、时间关系、事件关系等。可以说没有任何一种检索模式能够适用全部的情景。**混合检索通过多个检索系统的组合，实现了多个检索技术之间的互补。**



## 面向大语言模型的检索增强生成技术：调查


### 摘要

大型语言模型 (大语言模型，LLMs) 虽展现出强大能力，但在实际应用中，例如在准确性、知识更新速度和答案透明度方面，仍存在挑战。**检索增强生成 (Retrieval-Augmented Generation, RAG) 是指在利用大型语言模型回答问题之前，先从外部知识库检索相关信息。**

RAG 被证明能显著提升答案的准确性，并特别是在知识密集型任务上减少模型的错误输出。通过引用信息来源，用户可以核实答案的准确性，从而增强对模型输出的信任。

RAG 在大型语言模型时代的发展模式，总结了三种模式：初级 RAG、高级 RAG 和模块化 RAG。

**为了解决纯参数化模型的局限，语言模型可以采取半参数化方法，将非参数化的语料库数据库与参数化模型相结合。这种方法被称为检索增强生成（Retrieval-Augmented Generation, RAG）**

### 引言

#### LLM的不足

-   可能产生不准确的信息，并在处理特定领域或高度专业化的查询时表现出知识缺失
-   当所需信息超出模型训练数据范围或需要最新数据时，大型语言模型可能无法提供准确答案
-   通常对模型进行微调来适应特定领域或专有信息，从而将知识参数化，但它需要消耗大量计算资源，成本高昂，且需要专业技术知识，难以适应不断变化的信息环境
-   LLM将大量语料库中学习到知识储存在模型参数中，它难以保留训练语料库中的所有知识，特别是对于那些不太常见且具体的知识
-   LLM模型参数无法动态更新，参数化知识可能会过时
-   参数的增加会导致训练和推理的计算成本增加
-   LLM会出现幻觉

#### 解决办法

将 RAG 引入大模型的上下文学习 (In-Context Learning, ICL) 中，可以有效减轻上述问题，这种方法具有明显且易于实施的效果。在推理过程中，RAG 动态地从外部知识源中检索信息，并利用这些检索到的数据作为组织答案的参考。这极大地提高了答案的准确性和相关性，有效地解决了大语言模型中的幻觉式错误问题。这项技术自大语言模型出现以来迅速受到关注，已成为提升聊天机器人效能和增强大语言模型实用性的前沿技术之一。RAG 通过将事实知识与大语言模型的训练参数分离，巧妙地结合了生成模型的强大功能和检索模块的灵活性，为纯参数化模型中固有的知识不完整和不充分问题提供了有效的解决方案。



#### RAG算法和模型的发展

将 RAG 引入大模型的上下文学习 (In-Context Learning, ICL) 中，可以有效减轻上述问题，这种方法具有明显且易于实施的效果。在推理过程中，RAG 动态地从外部知识源中检索信息，并利用这些检索到的数据作为组织答案的参考。这极大地提高了答案的准确性和相关性，有效地解决了大语言模型中的幻觉式错误问题。这项技术自大语言模型出现以来迅速受到关注，已成为提升聊天机器人效能和增强大语言模型实用性的前沿技术之一。RAG 通过将事实知识与大语言模型的训练参数分离，巧妙地结合了生成模型的强大功能和检索模块的灵活性，为纯参数化模型中固有的知识不完整和不充分问题提供了有效的解决方案。



### RAG背景

在大语言模型 (Large Language Models) 的领域中，RAG 特指一种模式：模型在回答问题或生成文本时，首先从广阔的文档库中寻找相关信息。然后，模型使用这些找到的信息来生成回答或文本，从而提高其预测的准确度。RAG 的方法使得开发人员无需为每一个特定任务重新训练整个庞大的模型。他们可以简单地给模型加上一个知识库，通过这种方式增加模型的信息输入，从而提高回答的精确性。

RAG的两个阶段

1.   使用编码模型根据问题找到相关的文档
2.   生成阶段：以找到的上下文为基础，生成文本

#### RAG和微调的对比

![RAG_FT](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/RAG_FT.png)



RAG 和微调可以相互补充，而非相互排斥，从而在不同层次上增强模型的能力。

表 1: RAG 与微调之间的对比

| 特征比较       | RAG                                                          | 微调 (Fine-tuning)                                           |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 知识更新       | 直接更新检索知识库，确保信息持续更新，无需频繁重新训练，非常适合动态变化的数据环境。 | 存储静态数据，对于知识和数据的更新需要重新训练。             |
| 外部知识       | 擅长利用外部资源，特别适合处理文档或其他结构化/非结构化数据库。 | 可用于将预训练中外部学习到的知识与大型语言模型保持一致，但对于频繁变化的数据源可能不太实用。 |
| 数据处理       | 对数据的处理和操作要求极低。                                 | 依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。 |
| 模型定制       | 侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。 | 允许根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。 |
| 可解释性       | 答案能够追溯到具体的数据来源，提供更高的可解释性和可追踪性。 | 就像一个黑盒子，并不总是清楚模型为什么会做出某种反应，可解释性相对较低。 |
| 计算资源       | 需要计算资源来支持检索策略和数据库相关技术。外部数据源的整合和更新需保持维护。 | 有必要准备和整理高质量的训练数据集，确定微调目标，并提供相应的计算资源。 |
| 延迟要求       | 因涉及数据检索，可能带来较高的延迟。                         | 经过微调的大语言模型 (LLM) 可以不通过检索直接回应，降低延迟。 |
| 降低幻觉       | 由于每个回答都基于检索到的实际证据，因此本质上更不容易产生错误或虚构的回答。 | 根据特定领域的数据训练模型，有助于减少幻觉，但面对未训练过的输入时仍可能出现幻觉。 |
| 伦理和隐私问题 | 从外部数据库存储和检索文本可能引起伦理和隐私方面的担忧       | 训练数据中的敏感内容可能会引起伦理和隐私方面的问题。         |

### RAG框架

#### Naive RAG

原始 RAG 的流程包括传统的索引、检索和生成步骤。原始 RAG 也被概括为一个“检索” - “阅读”框架 。

**索引**

指的是在离线状态下，从数据来源处获取数据并建立索引的过程。具体而言，构建数据索引包括以下步骤：

1.  **数据索引：** 包括清理和提取原始数据，将 PDF、HTML、Word、Markdown 等不同格式的文件转换成纯文本。
2.  **分块：** 将加载的文本分割成更小的片段。由于语言模型处理上下文的能力有限，因此需要将文本划分为尽可能小的块。
3.  **嵌入和创建索引：** 这一阶段涉及通过语言模型将文本编码为向量的过程。所产生的向量将在后续的检索过程中用来计算其与问题向量之间的相似度。由于需要对大量文本进行编码，并在用户提问时实时编码问题，因此嵌入模型要求具有高速的推理能力，同时模型的参数规模不宜过大。完成嵌入之后，下一步是创建索引，**将原始语料块和嵌入以键值对形式存储**，以便于未来进行快速且频繁的搜索。

**检索：**

根据用户的输入，采用与第一阶段相同的编码模型将查询内容转换为向量。系统会计算问题向量与语料库中文档块向量之间的相似性，并根据相似度水平选出最相关的前 K 个文档块作为当前问题的补充背景信息。

**生成：**

将给定的问题与相关文档合并为一个新的提示信息。随后，大语言模型（LLM）被赋予根据提供的信息来回答问题的任务。根据不同任务的需求，可以选择让模型依赖自身的知识库或仅基于给定信息来回答问题。如果存在历史对话信息，也可以将其融入提示信息中，以支持多轮对话。

**Naive RAG 的挑战：**

Naive RAG 主要在三个方面面临挑战：检索质量、回应生成质量和增强过程。

-   **检索质量：** 
    -   最主要的问题是低精度，即检索集中的文档块并不都与查询内容相关，这可能导致信息错误或不连贯。
    -   其次是低召回率问题，即未能检索到所有相关的文档块，使得大语言模型无法获取足够的背景信息来合成答案。
    -   此外，数据冗余或过时可能导致检索结果不准确。
-   **回应生成质量：** 
    -   制造错误信息，即模型在缺乏足够上下文的情况下虚构答案。
    -   回答不相关，即模型生成的答案未能针对查询问题。
    -   生成有害或偏见性回应。
-   **增强过程：** 
    -   如何将检索到的文段的上下文有效融入当前的生成任务。如果处理不得当，生成的内容可能显得杂乱无章。
    -   当多个检索到的文段包含相似信息时，冗余和重复成为问题，这可能导致生成内容的重复。
    -   如何判断多个检索到的文段对生成任务的重要性或相关性，增强过程需要恰当地评估每个文段的价值。
    -   检索到的内容可能具有不同的写作风格或语调，增强过程需调和这些差异，以确保最终输出的一致性。
    -   生成模型可能会过度依赖于增强信息，导致生成的内容仅是重复检索到的信息，而缺乏新的价值或综合信息。

#### 高级 RAG

为了克服 Naive RAG 的局限性，高级 RAG 进行了针对性的改进。在检索生成质量方面，高级 RAG 引入了预检索和后检索的方法。它还通过滑动窗口、细粒度分割和元数据等手段优化了索引，以解决 Naive RAG 所遇到的索引问题。同时，高级 RAG 也提出了多种优化检索流程的方法。在具体实施上，高级 RAG 可以通过流水线方式或端到端的方式进行调整。

**预检索处理**

-   **优化数据索引** 优化数据索引旨在提高索引内容的质量。目前主要采用五种策略：提升索引数据粒度、优化索引结构、增加元数据、对齐优化以及混合检索。
    1.  **提升数据粒度：** 在索引前的优化是为了改进文本的标准化和一致性，确保事实准确和上下文丰富，从而保障 RAG 系统的表现。
        -   文本标准化意在剔除无关信息和特殊字符，提升检索效率。
        -   在确保一致性方面，主要是消除术语和实体的歧义，剔除重复或冗余信息，简化检索过程。
        -   事实的准确性至关重要，应尽可能验证每项数据。
        -   在保持上下文方面，通过添加特定领域的注释和用户反馈循环不断更新，使系统适应现实世界的交互上下文。
        -   考虑时间敏感性，应更新过时文档。
    2.  **优化索引结构：** 通过调整数据块大小、改变索引路径和加入图结构信息可以实现这一目标。调整数据块大小的方法是尽可能多地收集相关上下文，同时尽量减少干扰。在构建 RAG 系统时，块大小是关键参数。不同的评估框架用于比较不同块的大小。[LlamaIndex](https://www.llamaindex.ai/) 利用 GPT4 评估数据的保真度和相关性，LLaMA[Touvron *et al.*, 2023] 索引能自动评估不同块化方法的效果。跨多个索引路径查询与之前的元数据过滤和块化方法紧密相关，可能涉及同时在不同索引中进行查询。标准索引可用于特定查询，或使用独立索引基于元数据关键词（如“日期”索引）进行搜索或过滤。引入图结构是将实体转化为节点，将它们之间的关系转化为关联，这可以通过利用节点间的关系来提高对多跳问题的准确性。使用图数据索引能提高检索的相关性。
    3.  **添加元数据信息：** 这一策略的核心是将引用的元数据，如日期和用途（用于筛选）等，嵌入到数据块中。添加如章节和引用小节等元数据，对于提升检索效率是有益的。当索引被分割成多个块时，如何高效检索便成为关键。通过元数据进行初步筛选可以提高检索的效率和准确性。
    4.  **对齐优化：** 该策略主要针对不同文档之间的对齐问题和差异。对齐的核心思想是引入“假设性问题”，即创建适合用每篇文档回答的问题，并将这些问题与文档结合起来。这种做法有助于解决文档间的对齐问题和不一致性。
    5.  **混合检索：** 混合检索的优势在于它结合了不同检索技术的长处。它智能地融合了关键词搜索、语义搜索和向量搜索等多种技术，适应不同类型的查询需求，确保能够一致地检索到最相关和内容丰富的信息。**混合检索作为检索策略的重要补充，能够显著提升 RAG 流程的整体性能。**

**嵌入 (Embedding)**

-   **微调嵌入：** 微调嵌入模型的调整直接影响到 RAG 的有效性。微调的目的是让检索到的内容与查询之间的相关性更加紧密。微调嵌入的作用可以比作在语音生成前对“听觉”进行调整，优化检索内容对最终输出的影响。通常，微调嵌入的方法可以分为针对特定领域上下文的嵌入调整和检索步骤的优化。特别是在处理不断变化或罕见术语的专业领域，这些定制化的嵌入方法能够显著提高检索的相关性。BGE[BAAI, 2023]嵌入模型是一个经过微调的高性能嵌入模型，例如由 BAAI 3 开发的 BGE-large-EN。为了对 BGE 模型进行微调，首先使用诸如 gpt-3.5-turbo 这样的大语言模型（LLM）根据文档块制定问题，其中问题和答案（文档块）构成了微调过程中的训练对。
-   **动态嵌入（Dynamic Embedding）**：不同于静态嵌入（static embedding），动态嵌入根据单词出现的上下文进行调整，为每个单词提供不同的向量表示。例如，在 Transformer 模型（如 BERT）中，同一单词根据周围词汇的不同，其嵌入也会有所变化。研究发现，在 OpenAI 的 text-embeddingada-002 模型中，文本长度小于 5 个 Token 时，常出现意外高的余弦相似度。理想的嵌入应该包含足够的上下文，以保证良好的结果。OpenAI 的 embeddings-ada-02 是基于大语言模型（如 GPT）原理开发的，比传统静态嵌入模型更复杂，能够捕捉一定程度的上下文。尽管它在上下文理解方面表现出色，但可能不如最新的大型语言模型（如 GPT-4）那样对上下文敏感。

**检索后处理流程**

在从数据库中检索出有价值的上下文后，将其与查询内容合并输入到大语言模型（LLM）会遇到挑战。一次性向大语言模型展示所有相关文档可能会超出其处理的上下文窗口限制。将多个文档拼接成一个冗长的检索提示不仅效率低，还会引入噪声，影响大语言模型聚焦关键信息。因此，需要对检索到的内容进行额外处理，以解决这些问题。

-   **ReRank（重新排序）**：重新排序，将最相关的信息置于提示的前后边缘，是一个简单直接的方法。

-   **Prompt 压缩**：研究显示，检索文档中的噪音会对 RAG 性能产生不利影响。在处理的后期阶段，我们主要关注于压缩无关紧要的上下文，凸显关键段落，并缩短整体的上下文长度。

    不过，在 RAG 或者长篇上下文的情境中，这些方法可能会遗失关键信息。

-   **RAG 管道优化：** 检索过程的优化旨在提升 RAG 系统的效率和信息质量。当前的研究主要集中在智能结合不同的搜索技术，优化检索步骤，引入认知回溯概念，灵活运用多样化的查询策略，并利用嵌入式相似度。这些努力共同追求在 RAG 检索中达到效率与上下文信息丰富度的平衡。

-   **混合搜索的探索：** RAG 系统巧妙结合了基于关键词、语义以及向量的多种搜索技术。这种综合方法让 RAG 系统能够应对不同的查询类型和信息需求，有效地获取最相关且内容丰富的信息。混合搜索作为一种强大的补充手段，显著提升了 RAG 流程的整体表现。

-   **递归检索与查询引擎：** 在 RAG 系统中，采用递归检索和高级查询引擎是提高检索效果的另一有效手段。递归检索的首要步骤是在初始阶段获取小型文档块，以便抓住关键语义。随后，该过程会提供更大的文档块，为大语言模型 (LM) 提供更丰富的上下文信息。这种双重检索策略既保证了效率，又能提供深入的上下文回应。

-   **StepBack-prompt 方法：** 集成到 RAG 流程中的 StepBack-prompt 方法[Zheng *et al.*, 2023] 促使大语言模型 (LLM) 在处理具体案例时能够退一步，转而思考背后的普遍概念或原则。研究发现，这种结合后向提示的方法在处理各种复杂、推理密集的任务时表现卓越，充分展现了其与 RAG 的良好兼容性。这种方法既能用于后向提示的答案生成，也能用于最终的问答环节。

-   **子查询：** 根据不同场景，我们可以采取多种查询策略，如使用 LlamaIndex 等框架提供的查询引擎、树状查询、向量查询或基本的块序列查询。

-   **HyDE 方法：** 这种方法基于一个假设：相较于直接查询，通过大语言模型 (LLM) 生成的答案在嵌入空间中可能更为接近。HyDE 首先响应查询生成一个假设性文档（答案），然后将其嵌入，并利用此嵌入去检索与假设文档类似的真实文档。这种方法强调答案之间的嵌入相似性，而非单纯依赖于查询的嵌入相似性。但在某些情况下，特别是当语言模型对话题不够熟悉时，它可能导致错误实例的增加。



#### 模块化 RAG

模块化 RAG 结构打破了传统的 RAG”框架，这个框架原本涉及索引、检索和生成，现在提供了更广泛的多样性和更高的灵活性。它不仅集成了各种方法来丰富功能模块，比如在相似性检索中加入了搜索模块，并且在检索器中采用了微调 (fine-tuning) 策略。特别的问题也催生了重构后的 RAG 模块，以及类似的迭代方法。这种模块化的 RAG 范式正逐渐成为 RAG 领域的趋势，它支持从序列化流程到跨多个模块的端到端训练方法。三种 RAG 范式的对比在图 3 中进行了详细展示。

![fram_compare](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/fram_compare.png)

图 3：三种 RAG 范式的比较

**新模块（NEW Modules）**

-   **搜索模块(Search)：** 与简单/高级 RAG 的查询和语料间的常规相似性检索不同，这个特定场景下的搜索模块融合了直接在（附加的）语料库中进行搜索的方法。这些方法包括利用大语言模型（LLM）生成的代码、SQL、Cypher 等查询语言，或是其他定制工具。其搜索数据源多样，涵盖搜索引擎、文本数据、表格数据或知识图等。

-   **记忆模块：** 本模块充分利用大语言模型本身的记忆功能来引导信息检索。其核心原则是寻找与当前输入最为匹配的记忆。

-   **额外生成模块：** 面对检索内容中的冗余和噪声问题，这个模块通过大语言模型生成必要的上下文，而非直接从数据源进行检索。通过这种方式，由大语言模型生成的内容更可能包含与检索任务相关的信息。

-   **任务适应模块：** 该模块致力于将 RAG 调整以适应各种下游任务。

-   **对齐模块：** 在 RAG 的应用中，查询与文本之间的对齐一直是影响效果的关键因素。在模块化 RAG 的发展中，研究者们发现，在检索器中添加一个可训练的 Adapter 模块能有效解决对齐问题。

-   **验证模块：** 在现实世界中，我们无法总是保证检索到的信息的可靠性。检索到不相关的数据可能会导致大语言模型产生错误信息。因此，可以在检索文档后加入一个额外的验证模块，以评估检索到的文档与查询之间的相关性，这样做可以提升 RAG的鲁棒性。

    

**新模式（New Patterns）**

RAG 的组织方法具有高度灵活性，能够根据特定问题的上下文，对 RAG 流程中的模块进行替换或重新配置。在基础的 Naive RAG 中，包含了检索和生成这两个核心模块（有些文献中称之为阅读或合成模块），这个框架因而具备了高度的适应性和多样性。目前的研究主要围绕两种组织模式：一是增加或替换模块，二是调整模块间的工作流程。

-   **增加或替换模块** 在增加或替换模块的策略中，我们保留了原有的检索器 - 阅读器结构，同时加入新模块以增强特定功能。 
-   **调整模块间的工作流程** 在调整模块间流程的领域，重点在于加强语言模型与检索模型之间的互动。



### 检索器（Retriever）

在 RAG（检索增强生成）技术中，“R”代表检索，其作用是从大量知识库中检索出最相关的前 k 个文档。

#### 如何获得准确的语义表示？

在 RAG 中，语义空间指的是查询和文档被映射的多维空间。

进行检索时，我们是在这个语义空间内进行评估的。如果语义表达不准确，对 RAG 的影响将是灾难性的。下面是两种构建准确语义空间的方法。

**块优化**

处理外部文档的第一步是分块(chunk)，以获得更细致的特征。接着，这些文档块被嵌入（Embedded）。

Embedding  太大或太小的文本块可能无法取得最佳效果。因此，找到适合语料库文档的最佳块大小至关重要，以确保搜索结果的准确性和相关性。

选择分块策略时，需要考虑的要素包括：被索引内容的特点、使用的嵌入模型及其最适块大小、用户查询的预期长度和复杂度、以及检索结果在特定应用中的使用方式。例如，对于不同长度的内容，应选用不同的分块模型。不同的嵌入模型，如 Sentence-Transformer 和 text-embedding-ada-002，在处理不同大小的文本块时效果各异；例如，Sentence-Transformer 更适合单句处理，而 text-embedding-ada-002 更适合处理包含 256 或 512 Token 的文本块。用户问题文本的长度和复杂性，以及应用程序的特定需求（如语义搜索或问答），也会影响分块策略的选择。这可能与选用的大语言模型的 Token 限制直接相关，因此可能需要调整块大小。实际上，准确的查询结果是通过灵活应用多种分块策略来实现的，并没有最佳策略，只有最适合的策略。

当前的 RAG 研究采用了多种块优化方法，以提高检索的效率和准确性。其中，技术如滑动窗口技术通过多次检索，聚合全局相关信息，实现分层检索。

Small2big 技术在搜索过程中使用小文本块，并为语言模型提供更大的相关文本块进行处理。摘要嵌入（Abstract embedding）技术对文档摘要执行 Top K 检索，以提供完整的文档上下文。元数据过滤（Metadata Filtering）技术通过文档的元数据进行过滤。图索引（Graph Indexing）技术把实体和关系转化为节点和连接，这在处理多跳问题时显著提升了相关性。这些方法的结合显著提升了 RAG 的检索效果和性能。

**微调嵌入模型(Fine-tuning Embedding Models)**

在确定了 Chunk 的适当大小之后，我们需要通过一个嵌入模型（Embedding model）将 Chunk 和查询嵌入(Embedding )到语义空间中。因此，嵌入模型是否能有效代表整个语料库变得极其重要。如今，一些出色的嵌入模型已经问世，例如 UAE、Voyage、BGE 等，它们在大规模语料库上预训练过。但在特定领域中应用时，这些模型可能无法准确地反映领域特定的语料信息。此外，为了确保模型能够理解用户查询与内容的相关性，对嵌入模型进行任务特定的微调至关重要，否则未经微调的模型可能无法满足特定任务的需求。因此，对嵌入模型进行微调对于其下游应用是必不可少的。

-   **领域知识微调**：微调嵌入模型的主流方法使用的数据集包括查询（Queries）、语料库（Corpus）和相关文档（Relevant Docs）。嵌入模型基于查询在语料库中检索相关文档，然后根据查询的相关文档是否命中作为衡量模型的标准。
-   **对下游任务的微调** ：

#### 如何协调查询和文档的语义空间

在 RAG 应用中，有些检索器用同一个嵌入模型来处理查询和文档，而有些则使用两个不同的模型。此外，用户的原始查询可能表达不清晰或缺少必要的语义信息。因此，协调用户的查询与文档的语义空间显得尤为重要。下面介绍两种关键技术，帮助实现这一目标。

-   **查询重写**（Query Rewrite）：一种直接的方式是对查询进行重写。利用大语言模型的能力生成一个指导性的伪文档，然后将原始查询与这个伪文档结合，形成一个新的查询。
-   **嵌入变换**（Embedding Transformation）：过在查询编码器后加入一个特殊的适配器，并对其进行微调，从而优化查询的嵌入表示，使之更适合特定的任务

#### 调整检索器结果以适应大语言模型的需求

在 RAG（Retrieval-Augmented Generation）流程中，即便我们采用各种技术提升检索效果，最终对 RAG 的整体性能可能仍无明显提升。原因在于检索到的文档可能并不符合大语言模型（LLM）的需求。下面介绍两种方法，以使检索器的输出更好地符合 LLM 的偏好。

-   **LLM 监督下的训练**：从大语言模型获取的反馈信号来调整嵌入（embedding）模型。
-   **插入适配器**：选择外接适配器来进行模型对齐。



### 生成器（Generator）

**在 RAG 系统中，生成器是核心部分之一，它的职责是将检索到的信息转化为自然流畅的文本。**这一设计灵感源自于传统语言模型，但不同于一般的生成式模型，RAG 的生成器通过利用检索到的信息来提高文本的准确性和相关性。在 RAG 中，生成器的输入不仅包括传统的上下文信息，还有通过检索器得到的相关文本片段。这使得生成器能够更深入地理解问题背后的上下文，并产生更加信息丰富的回答。此外，生成器还会根据检索到的文本来指导内容的生成，确保生成的内容与检索到的信息保持一致。正是因为输入数据的多样性，我们针对生成阶段进行了一系列的有针对性工作，以便更好地适应来自查询和文档的输入数据。

#### 如何通过后检索处理提升检索结果

对于未经微调的大型语言模型，多数研究依靠像 GPT-4 这样的知名大型语言模型，借助它们强大的内部知识库来全面检索文档信息。然而，这些大型模型仍然存在一些固有问题，比如上下文长度限制和对冗余信息的敏感性。为了解决这些问题，一些研究开始关注后检索处理。**后检索处理指的是，在通过检索器从大型文档数据库中检索到相关信息后，对这些信息进行进一步的处理、过滤或优化。其主要目的是提高检索结果的质量，更好地满足用户需求或为后续任务做准备。**可以将其理解为对检索阶段获得的文档进行二次处理。**后检索处理通常包括信息压缩和结果的重新排序。**

-   信息压缩（Information Compression）：信息压缩方面，即使检索器能够从庞大的知识库中提取相关信息，我们仍然面临处理大量检索文档信息的挑战。一些研究试图通过扩大大型语言模型的上下文长度来解决这个问题，但当前的大模型还是受到上下文限制。在这种情况下，进行信息浓缩变得必要。总体来说，**信息浓缩的重要性主要体现在减少信息噪音、解决上下文长度限制和提升生成效果等方面**。

-   重新排序（Rerank）：在文档重排过程中，重排模型的主要作用是优化由检索器检索出的文档集合。

    当大语言模型 (LLM) 面临额外上下文的添加时，其性能往往会下降。为了应对这一挑战，重排序被提出作为一种行之有效的策略。**其核心在于对文档记录进行重新组织，优先安排最相关的内容位于前列，同时将文档总量控制在一定数量之内。**这种做法不仅有效缓解了检索时可能出现的上下文窗口扩大问题，也显著提升了检索的效率和响应速度。重排序过程中引入的上下文压缩功能，目的是基于特定查询上下文直接筛选出相关信息。这一策略的独特之处在于，通过减少每个文档的内容量和筛选掉不相关的文档，它能更加集中地展示检索结果中的关键信息。因此，重排序模型在整个信息检索过程中起到了优化和精化的作用，为后续大语言模型的处理提供了更加有效和精准的输入。

#### 如何优化生成器应对输入数据？

在 RAG 模型中，优化生成器是至关重要的。生成器负责将检索到的信息转化为相关文本，形成模型的最终输出。其优化目的在于确保生成文本既流畅又能有效利用检索文档，更好地回应用户的查询。

在一般的大语言模型 (LLM) 生成任务中，输入通常是个查询。而 RAG 的不同之处在于，输入不仅包括查询，还涵盖了检索器找到的多种文档（无论是结构化还是非结构化）。额外信息的加入对模型理解尤其是小型模型造成显著影响，因此，针对查询和检索文档的输入进行模型微调变得尤为重要。一般在将输入提供给微调过的模型之前，需要对检索器找到的文档进行后续处理。值得注意的是，RAG 中对生成器的微调方式与大语言模型的普通微调方法大体相同。

-   通用优化过程：通用优化过程涉及训练数据中的输入输出对，目的是让模型学会根据输入 x 生成输出 y。
-   运用对比学习：在训练数据准备阶段，通常会生成输入和输出之间的交互对，以此进行对比学习。



### Augmentation in RAG



## 参考链接

-   [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 论文原文](https://arxiv.org/pdf/2005.11401.pdf)
-   [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 论文阅读笔记](https://zhuanlan.zhihu.com/p/671448677)
-   [Retrieval-Augmented Generation for Open-Domain QA](https://zhuanlan.zhihu.com/p/339942960)
-   [检索、提示：检索增强的（Retrieval Augmented）自然语言处理](https://zhuanlan.zhihu.com/p/470784563)
-   [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 笔记](https://www.cnblogs.com/MTandHJ/p/17562977.html)
-   [举一反三：示例增强的（example augmented）自然语言处理](https://zhuanlan.zhihu.com/p/512969250)
-   [Langchain-Chatchat项目地址](https://github.com/chatchat-space/Langchain-Chatchat)
-   [Dify项目地址](https://github.com/langgenius/dify)
-   [Devv AI 是如何构建高效的 RAG 系统的](https://us.v2ex.com/t/1000319)
-   [Devv AI 是如何构建高效的 RAG 系统的 Part 2](https://typefully.com/Tisoga/PBB58Vu)
-   [面向大语言模型的检索增强生成技术：调查 [译]](https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey)

-   [大模型主流应用RAG的介绍——从架构到技术细节](https://luxiangdong.com/2023/09/25/ragone/#/RAG%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82%E6%A6%82%E8%A7%88)
-   [RAG知识目录](https://luxiangdong.com/2023/12/28/torchv1/#/3%EF%BC%9ARAG%E7%9F%A5%E8%AF%86%E7%9B%AE%E5%BD%95)
