# 大模型推理框架学习记录

下文中关于star，Issues, PR是截止2024-4-22的数据。

Issues和PR的格式为：open的数据/Closed的数量

## vLLM

Github： [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm) 

Star: 18.1k , Issues: 673/ 1796 , PR: 193/1409

### 简介

vLLM是一个开源的大模型推理加速框架，通过PagedAttention高效地管理attention中缓存的张量，实现了比HuggingFace Transformers高14-24倍的吞吐量。

PagedAttention 是 vLLM 的核心技术，它解决了LLM服务中内存的瓶颈问题。传统的注意力算法在自回归解码过程中，需要将所有输入Token的注意力键和值张量存储在GPU内存中，以生成下一个Token。这些缓存的键和值张量通常被称为KV缓存。

### 主要特性：

-   最高的服务吞吐量
-   使用 PagedAttention 高效管理注意力键和值内存
-   量化：GPTQ、AWQ、SqueezeLLM、FP8 KV 缓存
-   支持张量并行推理
-   支持流式输出
-   兼容 OpenAI 的接口服务
-   与 HuggingFace 模型无缝集成



## Huggingface TGI

Github: [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference)

star: 7.8k  Issues:  129/890   PR: 23/706

### 简介

Text Generation Inference（TGI）是 HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理。

### 主要特性

-   支持张量并行推理
-   支持传入请求 Continuous batching 以提高总吞吐量

-   使用 flash-attention 和 Paged Attention 在主流的模型架构上优化用于推理的 transformers 代码。**注意：并非所有模型都内置了对这些优化的支持。**
-   使用bitsandbytes(LLM.int8())和GPT-Q进行量化
-   内置服务评估，可以监控服务器负载并深入了解其性能
-   轻松运行自己的模型或使用任何 HuggingFace 仓库的模型
-   自定义提示生成：通过提供自定义提示来指导模型的输出，轻松生成文本
-   使用 Open Telemetry，Prometheus 指标进行分布式跟踪



## FasterTransformer

Github: [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

star: 5.4k  Issues: 246/376   PR: 41/128

### 简介

NVIDIA FasterTransformer (FT) 是一个用于实现基于Transformer的神经网络推理的加速引擎。它包含Transformer块的高度优化版本的实现，其中包含编码器和解码器部分。使用此模块，您可以运行编码器-解码器架构模型（如：T5）、仅编码器架构模型（如：BERT）和仅解码器架构模型（如： GPT）的推理。

FT框架是用C++/CUDA编写的，依赖于高度优化的 cuBLAS、cuBLASLt 和 cuSPARSELt 库，这使您可以在 GPU 上进行快速的 Transformer 推理。

与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理。

在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等。因此，使用FasterTransformer，您可以在多个 GPU 上以张量并行运行大型Transformer，以减少计算延迟。同时，TP 和 PP 可以结合在一起，在多 GPU 节点环境中运行具有数十亿、数万亿个参数的大型 Transformer 模型。

除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点。



### 存在的问题

-   英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了。



## TensorRT-LLM 





## 参考链接

-   [知乎问题：目前业界大模型推理框架很多，各有什么优缺点，应该如何选择？](https://www.zhihu.com/question/625415776/answer/3243562246)

