# 度小满轩辕大模型训练记录

**度小满金融大模型-轩辕6B复现**

XuanYuan-6B采用类LLaMA的模型架构。

GitHub项目地址： [GitHub - Duxiaoman-DI/XuanYuan: 轩辕：度小满中文金融对话大模型](https://github.com/Duxiaoman-DI/XuanYuan)

数据集地址： https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus

XuanYuan-6B 技术报告： [XuanYuan/xuanyuan_6b_report.md at main · Duxiaoman-DI/XuanYuan · GitHub](https://github.com/Duxiaoman-DI/XuanYuan/blob/main/xuanyuan_6b_report.md)

度小满轩辕大模型 PT 使用的是 DeepSpeed框架， A800 80G GPU 和 ZeRO-1级别进行训练。在我们的服务器上只能使用多机多卡（一台服务器A100 40G * 2，一台服务器 V100 32G * 2）ZeRO-3-Offload才能训练起来，但是执行时间非常久，训练完需要11年。ZeRO-3-offload 相关代码参考自Llama-Chinese项目，项目地址为：[GitHub - LlamaFamily/Llama-Chinese: Llama中文社区，最好的中文Llama大模型，完全开源可商用](https://github.com/LlamaFamily/Llama-Chinese)

## 环境搭建

具体可查看：[分布式训练环境搭建](./分布式训练环境搭建.md)

### 运行环境

Nvidia显卡驱动版本: 535.154.05
CUDA版本: 12.2
Python版本:3.10.13
gcc版本:11.2.1

## 下载模型和数据集和项目

设置代理：

```bash
git config --global https.proxy http://127.0.0.1:10809
git config --global http.proxy http://127.0.0.1:10809
```

设置huggingface token:

```bash
# 安装huggingface_hub
pip install --upgrade huggingface_hub
# 登录huggingface, 打开 https://huggingface.co/settings/tokens 生成token，将生成的token粘贴到下面的命令的输入框
huggingface-cli login
```

![image-20240407093508139](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240407093508139.png)

下载模型

```bash
cd /data/models
git clone https://huggingface.co/meta-llama/Llama-2-7b-hf.git
```

下载数据集

```bash
cd /data/datasets
git clone https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus.git
```



## 训练

### 环境搭建

启用 GCC 11：

```bash
# 创建一个新session叫xy  xuanyuan的拼音首字母
screen -S xy
sudo scl enable devtoolset-11 bash
```

将`llm-code`目录下的`requirements.txt`文件先备份，然后将内容更新为下面的内容

```bash
argparse==1.4.0
datasets==2.16.1
transformers==4.37.1
sentencepiece==0.1.99
```

安装Python 依赖

```bash
pip install -r requirements.txt
```

安装 deepspeed 

```bash
# 编译 fuse_adam cpu_adam 并且加速编译
DS_BUILD_FUSED_ADAM=1   DS_BUILD_CPU_ADAM=1 pip install deepspeed==0.14.0 --global-option="build_ext" --global-option="-j96"
```

### 数据预处理

解压数据集

```bash
cd /data/datasets/FinCorpus
# 解压
gzip -d *.gz
```

修改预处理脚本

```bash
cd llm-code
vim data_preprocess_run.sh
```

修改路径,调整后的sh内容如下：

```bash
# Pretrain数据预处理
python3 pretrain_data_process.py \
    --model_name_or_path /data/models/Llama-2-7b-hf \
    --data_path /data/datasets/FinCorpus/data \
    --cache_dir ./cache/datasets \
    --save_dir ./data/FinCorpus_tokenized \
    --max_length 4096 \
    --num_proc 1024
```

执行数据预处理

```bash
sh data_preprocess_run.sh
```

报错`OSError: [Errno 24] Too many open files`

![image-20240408020939523](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240408020939523.png)

执行下面的命令，将资源或进程数调大一点，或者将`data_preprocess_run.sh`中的`num_proc`调小一点：

```bash
ulimit -n 4096
```

执行结果如下：

![image-20240408033535715](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240408033535715.png)

可以看出数据有一千两百万条数据。

![image-20240408082737528](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240408082737528.png)

整个脚本运行花了大约4个小时。



### 预训练

使用两台服务器进行多机多卡训练，其中服务器 A 为 A100 （40G）* 2 , 服务器 B 为 V100 (32G) * 2。

添加`hostfile.txt`内容如下，`slots`表示服务器上有多少张显卡，这个参数需要保持一致。

```
服务器A的IP slots=2
服务器B的IP slots=2
```

调整预训练脚本，内容如下，路径改成实际的。

```bash
# Pretrain模型训练

# deepspped 参数说明
# --hostfile 多机多卡训练服务器IP和显卡配置

# dxm_llm_main.py参数说明
# --data_path 数据所在位置
# --model_name_or_path 模型文件位置
# --save_name 模型保存位置
# --learning_rate 学习率，默认为5e-5
# --weight_decay Weight decay，默认为0.01
# --num_warmup_steps lr scheduler的warmup步数
# --seed  随机种子
# --train_mode 训练模式：pretrain表示预训练任务，sft表示指令微调任务
# --epochs 指定训练轮数
# --total_num_steps 总训练步数
# --gradient_accumulation_steps 梯度累积步数
# --per_device_train_batch_size 每个GPU训练的Batch size
# --max_length 最大长度
# --gradient_checkpointing 是否开启梯度检查点，默认不开启。开启可节省GPU内存占用
# --log_steps 每隔多少步记录一次日志
# --save_steps 每隔多少步保存一次模型
# --ds_offload_cpu  是否开启cpu offload
# --ds_zero_stage deepspeed的zero配置
# --ds_steps_per_print 每隔多少步输出一次deepspeed日志
# --local_rank 多机多卡情况下的local_rank
# --global_rank 多机多卡情况下的global_rank
#

deepspeed \
    --hostfile hostfile.txt \
    dxm_llm_main.py \
    --data_path data/FinCorpus_tokenized \
    --model_name_or_path /data/models/Llama-2-7b-hf \
    --save_name model/model-pretrained \
    -train_mode pretrain \
    --epochs 1 \
    --per_device_train_batch_size 4 \
    --max_length 4096 \
    --ds_zero_stage 3 \
    --log_steps 2 \
    --save_steps 40 \
    --gradient_checkpointing
```

调整`llm-code`目录下的`config.py`，调整 ZeRO 参数如下：

```python
def get_deepspeed_config(args):
    ds_config = {
        "train_micro_batch_size_per_gpu":args.per_device_train_batch_size,  # 每个GPU的batch_size
        "gradient_accumulation_steps":args.gradient_accumulation_steps,  # 梯度累积步数
        "steps_per_print": args.ds_steps_per_print,  # deepspeed输出中间log
        "zero_optimization": {
            "stage": args.ds_zero_stage,  # 指定zero stage，可选0,1,2,3
        },
        "scheduler": {
            "type": "WarmupDecayLR",  # 学习率衰减策略
            "params": {
                "total_num_steps": args.total_num_steps,
                "warmup_min_lr": 0,
                "warmup_max_lr": args.learning_rate,
                "warmup_num_steps": args.num_warmup_steps
            }
        },
        "optimizer": {
            "type": "Adam",  # 优化器
            "params": {
                "lr": args.learning_rate,  # 学习率
                "weight_decay": args.weight_decay,  # 权重衰减
            }
        },
        "fp16": {
            "enabled": True,  # 开启fp16半精度训练，V100不支持BF16，所以使用FP16
            # 下面的参数来自 Lllama2-chinese
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "initial_scale_power": 16,
            "hysteresis": 2,
            "min_loss_scale": 1,
            "fp16_opt_level": "O2"

        },
        "gradient_clipping": 1.0,  # 梯度裁剪
        "prescale_gradients": False,  # 是否在梯度更新前缩放梯度
        "wall_clock_breakdown": False,  # 是否输出deepspeed时间分析
    }
    if args.ds_zero_stage == 3:
        ds_config["zero_optimization"].update({
            "overlap_comm": True,
            "contiguous_gradients": True,
            "sub_group_size": 1e9,
            "reduce_bucket_size": "auto",
            "stage3_prefetch_bucket_size": "auto",
            "stage3_param_persistence_threshold": "auto",
            "stage3_max_live_parameters": 1e9,
            "stage3_max_reuse_distance": 1e9,
            "gather_16bit_weights_on_model_save": True,
            # ZeRO ++ 参数
            "zero_quantized_weights": True, # 是否使用量化零权重（qwZ）
            "zero_hpz_partition_size": 2, # hpZ（辅助分区）组中的Rank数，默认为1表示没有hpZ，理想的是每个节点的Rank数（GPU）
            "zero_quantized_gradients": True, # 是否使用量化零梯度（qgZ）
        })
    return ds_config

```

将服务器A作为主节点，要将模型和项目拷贝到服务器B节点。

首先在服务B上创建目录并修改权限：

```bash
mkdir -p /data/datasets
mkdir -p /data/models
chmod -R 766 /data/datasets
chmod -R 766 /data/models
```

在服务器A上执行拷贝模型，执行之前需要做SSH免密，SSH免密工作这里就不再赘述，可查看互联网上的教程，拷贝命令示例如下：

```bash
scp -rf /data/models/Llama-2-7b-hf root@服务器B的IP:/data/models
```

使用`rsync`工具同步项目代码，项目代码中包含了tokenizer之后的数据，`rsync`工具使用教程可参考: [rsync用法][1]

同步命令示例如下，需要将路径和IP换成实际的，注意`--exclude`要写相对路径，写绝对路径的话不生效。

```bash
rsync -avzh --delete \
--exclude 'llm-code/cache/*' \
--exclude '*.pyc' \
--exclude '.git/*' \
/data/workspace/XuanYuan root@服务器B的IP:/data/workspace/ \
```

我将同步的脚本放在了`llm-code`目录下的`sync.sh`中。

执行同步脚本：

```bash
bash sync.sh
```

执行训练脚本：

```bash
sh pretrain_run.sh
```

#### 问题1 Connection reset by peer.

运行报错`RuntimeError: [2] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer. This may indicate a possible application crash on rank 0 or a network set up issue.`：

```
Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.02s/it]
10.168.144.5: Traceback (most recent call last):
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 197, in <module>
10.168.144.5:     main()
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 133, in main
10.168.144.5:     model = get_ds_model(args, dataloader_dict)
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 57, in get_ds_model
10.168.144.5:     model, _, _, lr_scheduler = deepspeed.initialize(
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/__init__.py", line 176, in initialize
10.168.144.5:     engine = DeepSpeedEngine(args=args,
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 262, in __init__
10.168.144.5:     self._configure_distributed_model(model)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1157, in _configure_distributed_model
10.168.144.5:     self._broadcast_model()
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1077, in _broadcast_model
10.168.144.5:     dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
10.168.144.5:     return func(*args, **kwargs)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
10.168.144.5:     return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
10.168.144.5:     return fn(*args, **kwargs)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 205, in broadcast
10.168.144.5:     return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.168.144.5:     return func(*args, **kwargs)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1910, in broadcast
10.168.144.5:     work = group.broadcast([tensor], opts)
10.168.144.5: RuntimeError: [2] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer. This may indicate a possible application crash on rank 0 or a network set up issue.

```

看报错可能是由于修改了 hostname ,  ssh 免密出现问题，执行`ssh root@your_hostname` 之后再次训练报错` ncclInvalidArgument: Invalid value for an argument.`：

```
10.x.x.x [2024-04-09 18:31:50,463] [INFO] [logging.py:96:log_dist] [Rank 0] Using quantized gradients
10.x.x.x Traceback (most recent call last):
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 197, in <module>
10.x.x.x     main()
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 133, in main
10.x.x.x     model = get_ds_model(args, dataloader_dict)
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 57, in get_ds_model
10.x.x.x     model, _, _, lr_scheduler = deepspeed.initialize(
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/__init__.py", line 176, in initialize
10.x.x.x     engine = DeepSpeedEngine(args=args,
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 262, in __init__
10.x.x.x     self._configure_distributed_model(model)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1157, in _configure_distributed_model
10.x.x.x     self._broadcast_model()
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1077, in _broadcast_model
10.x.x.x     dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
10.x.x.x     return func(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
10.x.x.x     return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
10.x.x.x     return fn(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 205, in broadcast
10.x.x.x     return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.x.x.x     return func(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1910, in broadcast
10.x.x.x     work = group.broadcast([tensor], opts)
10.x.x.x torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:219, invalid argument, NCCL version 2.14.3
10.x.x.x ncclInvalidArgument: Invalid value for an argument.
10.x.x.x Last error:
10.x.x.x Invalid config blocking attribute value -2147483648
10.x.x.x Traceback (most recent call last):
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 197, in <module>
10.x.x.x     main()
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 133, in main
10.x.x.x     model = get_ds_model(args, dataloader_dict)
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 57, in get_ds_model
10.x.x.x     model, _, _, lr_scheduler = deepspeed.initialize(
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/__init__.py", line 176, in initialize
10.x.x.x     engine = DeepSpeedEngine(args=args,
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 262, in __init__
10.x.x.x     self._configure_distributed_model(model)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1157, in _configure_distributed_model
10.x.x.x     self._broadcast_model()
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1077, in _broadcast_model
10.x.x.x     dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
10.x.x.x     return func(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
10.x.x.x     return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
10.x.x.x     return fn(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 205, in broadcast
10.x.x.x     return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.x.x.x     return func(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1910, in broadcast
10.x.x.x     work = group.broadcast([tensor], opts)
10.x.x.x torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:219, invalid argument, NCCL version 2.14.3
10.x.x.x ncclInvalidArgument: Invalid value for an argument.
10.x.x.x Last error:
10.x.x.x Invalid config blocking attribute value -2147483648
10.x.x.x [2024-04-09 18:31:52,113] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 42228
10.x.x.x [2024-04-09 18:31:52,113] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 42229
10.x.x.x [2024-04-09 18:31:53,539] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/dis/bin/python', '-u', 'dxm_llm_main.py', '--local_rank=1', '--data_path', 'data/FinCorpus_tokenized', '--model_name_or_path', '/data/models/Llama-2-7b-hf', '--save_name', 'model/model-pretrained', '--train_mode', 'pretrain', '--epochs', '1', '--per_device_train_batch_size', '4', '--max_length', '4096', '--ds_zero_stage', '3', '--log_steps', '2', '--save_steps', '40', '--gradient_checkpointing'] exits with return code = 1

```

为了查看详细的日志，添加下面的环境变量到`~/.bashrc`中。

```bash
# deepspeed 训练 环境变量
export LOGLEVEL=DEBUG  # 设置pytorch的日志级别
export NCCL_DEBUG=INFO # 设置nccl的日志级别
export NCCL_IB_DISABL=1 # 关闭NCCL的InfiniBand通信
export NCCL_DESYNC_DEBUG=1  # 关闭NCCL的InfiniBand通信
export NCCL_ASYNC_ERROR_HANDLING=1  # 关闭NCCL的InfiniBand通信
export TORCH_CPP_LOG_LEVEL=INFO  # 设置torch c++的日志级别
export TORCH_DISTRIBUTED_DEBUG=DETAIL  # 设置torch distributed的日志级别
export NCCL_SOCKET_IFNAME=bond0  # 设置NCCL默认的网卡名称 
```

查看环境上的网卡名称来，可以通过`ip addr`命令查看，要选择IP跟服务器IP相同的网卡：

![image-20240411013802533](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240411013802533.png)在修改了环境变量之后，Screen 之前建立的session无法读取到新的环境变量，所以需要将之前的session删除掉，重新建一个。然后再次执行训练脚本。

```bash
# 删除名字为 xy 的session
screen -X -S xy quit
# 新建一个名字叫 xy 的session
screen -S xy
# 使环境变量生效
source ~/.bashrc
```

#### 问题2 ncclInvalidArgument: Invalid value for an argument

再次执行训练脚本

```bash
sh pretrain_run.sh
```

运行报错 `ncclInvalidArgument: Invalid value for an argument.Invalid config blocking attribute value -2147483648`

```text
10.x.x.x Last error:
10.x.x.x Invalid config blocking attribute value -2147483648
10.x.x.x Traceback (most recent call last):
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 197, in <module>
10.x.x.x     main()
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 133, in main
10.x.x.x     model = get_ds_model(args, dataloader_dict)
10.x.x.x   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 57, in get_ds_model
10.x.x.x     model, _, _, lr_scheduler = deepspeed.initialize(
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/__init__.py", line 176, in initialize
10.x.x.x     engine = DeepSpeedEngine(args=args,
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 262, in __init__
10.x.x.x     self._configure_distributed_model(model)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1157, in _configure_distributed_model
10.x.x.x     self._broadcast_model()
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1077, in _broadcast_model
10.x.x.x     dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
10.x.x.x     return func(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
10.x.x.x     return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
10.x.x.x     return fn(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 205, in broadcast
10.x.x.x     return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.x.x.x     return func(*args, **kwargs)
10.x.x.x   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1910, in broadcast
10.x.x.x     work = group.broadcast([tensor], opts)
10.x.x.x torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:219, invalid argument, NCCL version 2.14.3
10.x.x.x ncclInvalidArgument: Invalid value for an argument.
10.x.x.x Last error:
10.x.x.x Invalid config blocking attribute value -2147483648

```

在`vllm`项目中发现有人遇到了同样的问题[Issues 1726: NCCL error](https://github.com/vllm-project/vllm/issues/1726)。这个回答中提到可能是由于服务器上面有两个版本的`nccl`，需要将不需要的卸载掉。查看服务器上发现确实有两个版本的 `nccl`。

```bash
# 查看 nccl
pip list |grep nccl
# 卸载 nccl
pip uninstall nvidia-nccl-cu11==2.14.3 -y
```

![image-20240411015347869](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240411015347869.png)

再次执行训练脚本：

```bash
sh pretrain_run.sh
```

结果运行了几个小时运行脚本卡死了，而且服务器特别卡，重启了服务器。

#### 问题3 NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. 

重启后输入`nvidia-smie`查看显卡状态报错`NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.`。参考 [NVIDIA驱动失效简单解决方案：NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver. ](https://blog.csdn.net/wjinjie/article/details/108997692) 安装`dkms`解决了问题。

```bash
# 查看已安装驱动的版本信息 结果显示的为 nvidia-535.154.05
ls /usr/src | grep nvidia

# 安装 dkms
yum install -y dkms

# 重新加载驱动
dkms install -m nvidia -v 535.154.05
```

#### 问题4 RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.

再次运行训练脚本，在日志中发现V100服务器的节点有一个报错：

```
10.168.144.5: /root/miniconda3/envs/dis/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
10.168.144.5: /root/miniconda3/envs/dis/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
10.168.144.5:   warn("The installed version of bitsandbytes was compiled without GPU support. "
10.168.144.5: /root/miniconda3/envs/dis/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
10.168.144.5: /root/miniconda3/envs/dis/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
10.168.144.5:   warn("The installed version of bitsandbytes was compiled without GPU support. "
10.168.144.5: Traceback (most recent call last):
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 197, in <module>
10.168.144.5:     main()
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 130, in main
10.168.144.5:     dataloader_dict = get_dataloader_common(args)
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/model_hook.py", line 74, in get_dataloader_common
10.168.144.5:     sampler = DistributedSampler(train_dataset, shuffle=True, seed=args.seed)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/utils/data/distributed.py", line 68, in __init__
10.168.144.5:     num_replicas = dist.get_world_size()
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1492, in get_world_size
10.168.144.5:     return _get_group_size(group)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 785, in _get_group_size
10.168.144.5:     default_pg = _get_default_group()
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 940, in _get_default_group
10.168.144.5:     raise RuntimeError(
10.168.144.5: RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
10.168.144.5: Traceback (most recent call last):
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 197, in <module>
10.168.144.5:     main()
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/dxm_llm_main.py", line 130, in main
10.168.144.5:     dataloader_dict = get_dataloader_common(args)
10.168.144.5:   File "/data/litao/workspace/XuanYuan/llm-code/model_hook.py", line 74, in get_dataloader_common
10.168.144.5:     sampler = DistributedSampler(train_dataset, shuffle=True, seed=args.seed)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/utils/data/distributed.py", line 68, in __init__
10.168.144.5:     num_replicas = dist.get_world_size()
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1492, in get_world_size
10.168.144.5:     return _get_group_size(group)
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 785, in _get_group_size
10.168.144.5:     default_pg = _get_default_group()
10.168.144.5:   File "/root/miniconda3/envs/dis/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 940, in _get_default_group
10.168.144.5:     raise RuntimeError(
10.168.144.5: RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
```

和一个警告

```text
/root/miniconda3/envs/dis/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
10.168.144.5:   warn("The installed version of bitsandbytes was compiled without GPU support. "
```

提示`bitsandbytes`是CPU版本，不支持8比特优化，GPU量化等。

`bitsandbytes` 是一个轻量级 Python 包装器，用来调用CUDA 自定义函数，特别是 8 位优化器、矩阵乘法 (LLM.int8()) 以及 8 和 4 位量化函数。安装可查看：[bitsandbytes安装文档](https://huggingface.co/docs/bitsandbytes/main/en/installation)

重新安装`bitsandbytes`一直安装不成功，推测可能是`config.py`中ZeRO++参数会使用量化，将ZeRO++参数去掉之后重新执行训练脚本就没有这个警告了。但是前面这个报错依然存在。

查看日志发现在运行的时候 V100的节点 `NCCL_SOCKET_IFNAME`参数不对，V100 节点网卡实际上是`eno1`。

![image-20240411082710268](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240411082710268.png)

在 v100 服务器上使用`nvidia-smi`查看GPU状态出现了 问题3同样的问题。按照问题3的解决办法解决上面的问题。

#### 问题5 训练卡死

再次执行训练脚本，在 `Loadding dataset` 和 `loading Model`的日志打印后一直卡死。

![image-20240412030118310](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240412030118310.png)

![image-20240412025929338](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240412025929338.png)

通过查阅GitHub Issue。

-   [Fengshenbang-LM 123][2]

-   [LLaMA-Factory 1775][3]

-   [LLaMA-Factory 1683][4]

其中 [Fengshenbang-LM 123][2] 提到下面三种方法可以解决

>   1.  启动命令前增加了`OMP_NUM_THREADS=1 MKL_NUM_THREADS=1`，避免多线程导致死锁；
>   2.  去掉了加载数据时的tqdm；
>   3.  记在数据的DataLoader的drop_last设置为True，pin_memory设置为True，num_workers设置为0；

`DataLoader`示例如下：

```python
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.per_device_train_batch_size,
        num_workers=0,  # 指定16个核并行处理   这个可能会导致训练卡死
        sampler=sampler,
        pin_memory=True,
        drop_last=True,
        collate_fn=collator,
    )
```

实测没有用，还是卡死。

其中 [LLaMA-Factory 1683][4] 提到可以添加 `export NCCL_P2P_LEVEL=NVL`环境变量解决卡死的问题。

将下面的内容追加到` ~/.bashrc`中

```bash
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NCCL_P2P_LEVEL=NVL
```

再次执行脚本

```bash
# 删除名字为 xy 的session
screen -X -S xy quit
# 新建一个名字叫 xy 的session
screen -S xy
# 切换 GCC 11
sudo scl enable devtoolset-11 bash
# 使环境变量生效
source ~/.bashrc
# 同步代码
cd /data/workspace/XuanYuan/llm-code/
sh sync.sh
# 再次执行训练脚本
sh pretrain_run.sh
```

实测没有用，还是卡死。

然后就尝试使用`torchrun`运行手写数字识别的项目。结果还是卡死，说明不是项目的问题，是环境的问题。

在 [nccl issues 1092][5] 中看到可能是nccl和Pytorch版本冲突的问题导致的卡死。

查看 cuda 和 nccl 的版本：

```python
python
Python 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
[I debug.cpp:49] [c10d] The debug level is set to DETAIL.
t^H>>> torch.version.cuda
'12.1'
>>> torch.cuda.nccl.version()
(2, 18, 1)
>>> 
```

可以看出我们pytorch对应的cuda版本是12.1，nccl的版本是2.18.1。重新安装nccl之后就没有训练卡死了。:

```bash
# 卸载 nccl
rpm -qa |grep nccl |xargs yum remove -y
# 添加 cuda centos7 yum 源
sudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo
# 重新安装 nccl
sudo yum install libnccl-2.18.1-1+cuda12.1 libnccl-devel-2.18.1-1+cuda12.1 libnccl-static-2.18.1-1+cuda12.1 -y
```

#### 问题6 训练报错OOM

再次执行就可以训练了，但是会报 `CUDA OOM `内存不足的错误。将 `pretrain_run.sh`中`batch_size` 修改成 `1 ` 再试。

```sh
deepspeed \
    --hostfile hostfile.txt \
    dxm_llm_main.py \
    --data_path data/FinCorpus_tokenized \
    --model_name_or_path /data/models/Llama-2-7b-hf \
    --save_name model/model-pretrained \
    --train_mode pretrain \
    --epochs 1 \
    --per_device_train_batch_size 1 \
    --max_length 4096 \
    --ds_zero_stage 3 \
    --log_steps 2 \
    --save_steps 40 \
    --gradient_checkpointing
```

`batch_size`修改成`1`还是报错OOM。将`config.py`的`get_deepspeed_config`函数最后加上CPU offload 的配置。

```python
    if args.ds_offload_cpu == True:
        ds_config["zero_optimization"].update({
            "offload_optimizer": {
                "device": "cpu"
            },
            "offload_param": {
                "device": "cpu"
            }
        })
```

修改`pretrain_run.sh`，加上`--ds_offload_cpu`参数。

#### 问题 7 Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.

再次运行就没有报错 OOM了, 但是执行过程中会日志显示 loss 会变成 nan ，再执行一会儿报错`Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.` 

![image-20240412110239071](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240412110239071.png)

错误原因是因为优化器学习率太高了，将优化器改成：

```python
 "optimizer": {
            "type": "Adam",
            "params": {
                "lr": args.learning_rate,
                "weight_decay": args.weight_decay
            }
        }
```

跑了一个晚上跑了700个step.

![](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/image-20240416013032268.png)



## 参考链接

[1]: https://wangchujiang.com/linux-command/c/rsync.html	"rsync用法"
[2]: https://github.com/IDEA-CCNL/Fengshenbang-LM/issues/123	"分布式多机多卡训练卡住，超时后报错"
[3]: https://github.com/hiyouga/LLaMA-Factory/issues/1775	" deepspeed zero3 配置下 DPO 训练 会出现训练进程卡死的问题，这是怎么回事呢"

[4]: https://github.com/hiyouga/LLaMA-Factory/issues/1683	"使用accelerate和deepspeed进行多卡微调LLM卡住"
[5]: https://github.com/NVIDIA/nccl/issues/1092	"nccl issues 1092"

